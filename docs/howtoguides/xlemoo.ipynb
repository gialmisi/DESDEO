{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# How to use XLEMOO: Explainable Learnable Multiobjective Optimization\n\nXLEMOO is a hybrid evolutionary-ML approach that combines evolutionary algorithms\n(Darwinian mode) with interpretable machine learning (Learning mode) to find\nnear-Pareto optimal solutions while providing **explainability** about what makes\nsolutions good.\n\nThe core idea is that by training interpretable ML models on the population history,\nwe can extract human-readable rules describing what characterizes high-performing\nsolutions. These rules can then be used to instantiate new candidate solutions\ndirectly.\n\n**Reference:**\nMisitano, G. (2024). Towards Explainable Multiobjective Optimization: XLEMOO.\n*ACM Trans. Evol. Learn. Optim.*, 4(1). https://doi.org/10.1145/3626104\n\n**Prerequisites:** Before proceeding, make sure you are familiar with:\n\n- [How evolutionary algorithms are structured in DESDEO](../../explanation/templates_and_pub_sub)\n- [How to configure evolutionary algorithms with the Pydantic interface](../ea_options)\n- [How multiobjective optimization problems are defined](../../explanation/problem_format.ipynb)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## How XLEMOO works\n\nXLEMOO alternates between two modes:\n\n**Darwinian mode** uses a standard evolutionary algorithm with crossover,\nmutation, and selection operators. The selection is done by a\n`ScalarizationSelector` that ranks solutions by a **scalarization column**\n(e.g., weighted sums or ASF) computed by the shared evaluator.\n\n**Learning mode** replaces the standard evolutionary operators with an ML-based\npipeline:\n\n1. Read population history from the archive.\n2. Rank solutions by the **same scalarization column** (computed by the evaluator\n   and stored in the archive) and split into a **H-group** (high-performing)\n   and **L-group** (low-performing).\n3. Train an interpretable ML classifier to distinguish H from L.\n4. Extract rules from the classifier and instantiate new candidate\n   solutions based on those rules.\n5. Evaluate the candidates using the shared evaluator.\n6. Select the best solutions using the same ScalarizationSelector.\n\nThe key design principle is that both modes use the **same fitness criterion**:\na scalarization function added to the Problem. The `EMOEvaluator` computes it\nfor every solution, and both the `ScalarizationSelector` and the\n`XLEMOOInstantiator` read the same column from the outputs/archive.\n\n```\nDarwinian mode (N iterations):\n  offspring       = crossover(population)\n  offspring       = mutate(offspring)\n  offspring_eval  = evaluator.evaluate(offspring)   # computes scalarization\n  population      = selection(parents, offspring)    # ranks by scalarization\n\nLearning mode (M iterations):\n  candidates      = instantiator(population)         # ML rules -> variables\n  candidates_eval = evaluator.evaluate(candidates)   # computes scalarization\n  population      = selection(parents, candidates)   # same ranking\n```\n\nBecause the evaluation step goes through the shared `EMOEvaluator`,\nall components (terminator, archives, etc.) are correctly notified about\nthe function evaluations performed during learning mode."
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Running XLEMOO with ASF-guided learning\n\nThe original XLEMOO paper recommends using the **Achievement Scalarizing\nFunction (ASF)** as the fitness criterion for the H/L group ranking. ASF\nincorporates a reference point provided by the decision maker, guiding\nthe learning mode to focus on solutions near that preferred region.\n\nWe use `xlemoo_options()` as the base configuration and customize the\n`ScalarizationSpec` to use ASF. We demonstrate on the DTLZ2 test problem\n(3 objectives, 6 variables). The true Pareto front of DTLZ2 lies on the\nunit sphere, and the distance variables ($x_3$ through $x_6$) should\nconverge to 0.5.\n\nThe parameters below match those used in the original XLEMOO experiments:\n- **SkopeRulesClassifier** as the ML model (the default in the paper)\n- **h_split=0.2, l_split=0.2** (top/bottom 20% for H/L groups)\n- **instantiation_factor=10** (generate 10x the population size in candidates)\n- **unique_only=True** (deduplicate before training)\n- **19 Darwinian + 1 Learning** iterations per cycle"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\n\nfrom desdeo.emo.options.algorithms import xlemoo_options\nfrom desdeo.emo.options.scalarization_selection import ScalarizationSpec\nfrom desdeo.emo.options.templates import emo_constructor\nfrom desdeo.emo.options.xlemoo_selection import XLEMOOSelectorOptions\nfrom desdeo.problem.testproblems import dtlz2\n\n# Define a 3-objective, 6-variable DTLZ2 problem\nproblem = dtlz2(n_objectives=3, n_variables=6)\n\n# Get pre-configured XLEMOO options\noptions = xlemoo_options()\n\n# Use ASF scalarization with a centered reference point\nreference_point = [0.5, 0.5, 0.5]\noptions.template.scalarization = ScalarizationSpec(\n    type=\"asf\",\n    symbol=\"scal_fitness\",\n    reference_point=reference_point,\n)\n\n# Configure learning mode to match the original paper\noptions.template.learning_instantiator = XLEMOOSelectorOptions(\n    ml_model_type=\"SkopeRules\",\n    ml_model_kwargs={\n        \"precision_min\": 0.1,\n        \"n_estimators\": 30,\n        \"max_depth\": None,\n    },\n    h_split=0.2,\n    l_split=0.2,\n    instantiation_factor=10.0,\n    unique_only=True,\n)\n\n# 19 Darwinian + 1 Learning per cycle (matching the original paper)\noptions.template.darwin_iterations_per_cycle = 19\noptions.template.learning_iterations_per_cycle = 1\n\n# Build and run the solver\nsolver, extras = emo_constructor(emo_options=options, problem=problem)\nresults = solver()\n\nprint(f\"Solutions found: {len(results.optimal_outputs)}\")\nprint(results.optimal_outputs.head())"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Visualizing the evolution\n\nRather than showing just the final Pareto front, we can animate the\n**selected population per generation** to see how the solutions evolve\nover time. The learning archive stores all evaluated solutions with\ngeneration numbers, and the base archive stores the selected population\nper generation in the `selections` attribute."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "import plotly.graph_objects as go\n\n# The learning archive stores selected populations per generation\nsel_df = extras.learning_archive.selections\n\n# Build animation frames — one per generation\ngenerations = sorted(sel_df[\"generation\"].unique().to_list())\n\nframes = []\nfor gen in generations:\n    gen_data = sel_df.filter(sel_df[\"generation\"] == gen)\n    frames.append(\n        go.Frame(\n            data=[\n                go.Scatter3d(\n                    x=gen_data[\"f_1\"].to_list(),\n                    y=gen_data[\"f_2\"].to_list(),\n                    z=gen_data[\"f_3\"].to_list(),\n                    mode=\"markers\",\n                    marker=dict(size=3, color=\"blue\", opacity=0.7),\n                )\n            ],\n            name=str(gen),\n        )\n    )\n\n# Initial frame\ninit = sel_df.filter(sel_df[\"generation\"] == generations[0])\n\nfig = go.Figure(\n    data=[\n        go.Scatter3d(\n            x=init[\"f_1\"].to_list(),\n            y=init[\"f_2\"].to_list(),\n            z=init[\"f_3\"].to_list(),\n            mode=\"markers\",\n            marker=dict(size=3, color=\"blue\", opacity=0.7),\n        )\n    ],\n    frames=frames,\n    layout=go.Layout(\n        title=\"XLEMOO population evolution on DTLZ2\",\n        scene=dict(\n            xaxis=dict(title=\"f_1\", range=[0, 1.5]),\n            yaxis=dict(title=\"f_2\", range=[0, 1.5]),\n            zaxis=dict(title=\"f_3\", range=[0, 1.5]),\n        ),\n        updatemenus=[\n            dict(\n                type=\"buttons\",\n                showactive=False,\n                buttons=[\n                    dict(\n                        label=\"Play\",\n                        method=\"animate\",\n                        args=[\n                            None,\n                            dict(\n                                frame=dict(duration=200, redraw=True),\n                                fromcurrent=True,\n                            ),\n                        ],\n                    ),\n                    dict(\n                        label=\"Pause\",\n                        method=\"animate\",\n                        args=[\n                            [None],\n                            dict(\n                                frame=dict(duration=0, redraw=False),\n                                mode=\"immediate\",\n                            ),\n                        ],\n                    ),\n                ],\n            )\n        ],\n        sliders=[\n            dict(\n                active=0,\n                steps=[\n                    dict(\n                        args=[[str(gen)], dict(frame=dict(duration=0, redraw=True), mode=\"immediate\")],\n                        label=str(gen),\n                        method=\"animate\",\n                    )\n                    for gen in generations\n                ],\n                currentvalue=dict(prefix=\"Generation: \"),\n            )\n        ],\n    ),\n)\nfig.show(renderer=\"notebook\", include_plotlyjs=\"cdn\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Verify proximity to unit sphere (final non-dominated front)\nobj_values = results.optimal_outputs[[\"f_1\", \"f_2\", \"f_3\"]].to_numpy()\nnorms = np.sqrt(np.sum(obj_values**2, axis=1))\nprint(f\"Median distance to unit sphere: {np.median(norms):.4f}\")\nprint(f\"Min: {norms.min():.4f}, Max: {norms.max():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Extracting and interpreting rules\n\nA key feature of XLEMOO is the ability to extract **human-readable rules**\nfrom the ML model that describe what characterizes good solutions. This is\nthe \"explainable\" part of XLEMOO.\n\nThe `XLEMOOInstantiator` (accessible via `extras.learning_instantiator`)\nstores the most recently trained classifier and its extracted rules. Since\nwe configured the learning mode with SkopeRules, `last_rules` contains the\nruleset and precisions from the final learning cycle — no need to re-train\na separate model.\n\nFor DTLZ2, the true Pareto-optimal solutions have $x_i = 0.5$ for the\ndistance variables ($i \\geq 3$), so we expect the rules to reflect\nconditions near $x_i \\approx 0.5$ for those variables."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Access the most recently trained SkopeRules model and its rules\ninstantiator = extras.learning_instantiator\nclassifier = instantiator.last_classifier\nruleset, precisions = instantiator.last_rules\n\nprint(f\"SkopeRules extracted {len(ruleset)} rules\\n\")\n\n# Display each rule with its precision (weight)\nvar_names = [f\"x_{i}\" for i in range(1, 7)]\n\nfor i, (rule, precision) in enumerate(zip(ruleset, precisions)):\n    print(f\"Rule {i + 1} (precision={precision:.3f}):\")\n    for (feat_name, op), threshold in rule.items():\n        # Map feature indices to variable names\n        import re\n        match = re.search(r\"(\\d+)$\", feat_name)\n        feat_idx = int(match.group(1)) if match else -1\n        var_label = var_names[feat_idx] if 0 <= feat_idx < len(var_names) else feat_name\n        print(f\"  {var_label} {op} {threshold}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "The rules above were extracted by the SkopeRules classifier during the\nmost recent learning mode cycle. Each rule represents a conjunction of\nconditions on decision variables, with a precision score indicating how\nwell the rule separates H-group from L-group solutions.\n\nFor the distance variables ($x_3$ through $x_6$), you should see\nconditions constraining values near 0.5, reflecting the true Pareto-optimal\nstructure of DTLZ2. The shape variables ($x_1$, $x_2$) will typically\nhave wider bounds since they parameterize the position on the Pareto front."
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## Interactive use: changing the reference point\n\nIn the original XLEMOO paper, the method is used interactively:\nthe decision maker provides a reference point, runs the optimization,\nexamines the extracted rules, and then may revise the reference point\nto explore a different region of the Pareto front.\n\nBelow, we run XLEMOO twice with different reference points (via the ASF\nscalarization) to show how the learning mode steers the search."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "import plotly.graph_objects as go\n\nref_points = {\n    \"centered\": [0.5, 0.5, 0.5],\n    \"biased toward f_1\": [0.2, 0.8, 0.8],\n}\n\nall_results = {}\n\nfor label, rp in ref_points.items():\n    problem = dtlz2(n_objectives=3, n_variables=6)\n    opts = xlemoo_options()\n    opts.template.termination.max_generations = 100\n    opts.template.darwin_iterations_per_cycle = 19\n    opts.template.learning_iterations_per_cycle = 1\n    opts.template.scalarization = ScalarizationSpec(\n        type=\"asf\",\n        symbol=\"scal_fitness\",\n        reference_point=rp,\n    )\n    opts.template.learning_instantiator = XLEMOOSelectorOptions(\n        ml_model_type=\"SkopeRules\",\n        ml_model_kwargs={\"precision_min\": 0.1, \"n_estimators\": 30},\n        h_split=0.2,\n        l_split=0.2,\n        instantiation_factor=10.0,\n        unique_only=True,\n    )\n\n    solver, extras = emo_constructor(emo_options=opts, problem=problem)\n    all_results[label] = solver()\n\n# Display objective ranges for each reference point\nfor label, res in all_results.items():\n    obj = res.optimal_outputs[[\"f_1\", \"f_2\", \"f_3\"]].to_numpy()\n    print(f\"Reference point: {ref_points[label]} ({label})\")\n    print(f\"  Objective ranges: {obj.min(axis=0).round(3)} to {obj.max(axis=0).round(3)}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "colors = [\"blue\", \"red\"]\n",
    "\n",
    "for (label, res), color in zip(all_results.items(), colors):\n",
    "    obj = res.optimal_outputs[[\"f_1\", \"f_2\", \"f_3\"]].to_numpy()\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=obj[:, 0], y=obj[:, 1], z=obj[:, 2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=3, color=color, opacity=0.6),\n",
    "            name=label,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"XLEMOO with different reference points on DTLZ2\",\n",
    "    scene=dict(xaxis_title=\"f_1\", yaxis_title=\"f_2\", zaxis_title=\"f_3\"),\n",
    ")\n",
    "fig.show(renderer=\"notebook\", include_plotlyjs=\"cdn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "The biased reference point `[0.2, 0.8, 0.8]` concentrates the H-group on\n",
    "solutions with low $f_1$, so the ML model learns rules that characterize\n",
    "that region. This demonstrates how XLEMOO can be steered interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "## Configuration reference\n\nThe scalarization function is configured via `ScalarizationSpec` on the template,\nwhile the ML learning mode is configured via `XLEMOOSelectorOptions`\n(`options.template.learning_instantiator`).\n\n### ScalarizationSpec (options.template.scalarization)\n\n| Parameter | Default | Description |\n|---|---|---|\n| `type` | `\"weighted_sums\"` | Scalarization type: `\"weighted_sums\"` or `\"asf\"` |\n| `symbol` | `\"scal_fitness\"` | Column name for the scalarization value |\n| `weights` | `None` (equal) | Weights per objective for weighted_sums |\n| `reference_point` | `None` | Reference point for ASF (required when type=\"asf\") |\n| `delta` | `0.000001` | Delta parameter for ASF |\n| `rho` | `0.000001` | Rho parameter for ASF |\n\n### XLEMOOSelectorOptions (options.template.learning_instantiator)\n\n| Parameter | Paper default | Description |\n|---|---|---|\n| `darwin_iterations_per_cycle` | 19 | EA generations per cycle before switching to learning mode |\n| `learning_iterations_per_cycle` | 1 | Learning mode iterations per cycle |\n| `ml_model_type` | `SkopeRules` | Interpretable ML classifier to use |\n| `h_split` | 0.2 | Fraction of best solutions for the H-group |\n| `l_split` | 0.2 | Fraction of worst solutions for the L-group |\n| `instantiation_factor` | 10.0 | How many candidates to generate (multiplier of population size) |\n| `generation_lookback` | 0 | How many recent generations to use (0 = all history) |\n| `ancestral_recall` | 0 | Earliest generations to always include |\n| `unique_only` | True | Deduplicate solutions before training |\n\n### Supported ML models\n\n- `SkopeRules` (from [imodels](https://github.com/csinva/imodels)) — default, generates interpretable rule sets\n- `DecisionTree` (scikit-learn `DecisionTreeClassifier`) — fast, easily visualized\n- `Slipper` (from imodels) — boosted rule learning\n- `BoostedRules` (from imodels) — boosted rule ensembles"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "## Customizing the ML model\n\nYou can change the ML model and pass additional keyword arguments to its\nconstructor. Below we use a `DecisionTree` with fewer generations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "problem = dtlz2(n_objectives=3, n_variables=6)\n\noptions = xlemoo_options()\noptions.template.termination.max_generations = 50\noptions.template.darwin_iterations_per_cycle = 19\noptions.template.learning_iterations_per_cycle = 1\noptions.template.scalarization = ScalarizationSpec(\n    type=\"asf\",\n    symbol=\"scal_fitness\",\n    reference_point=[0.5, 0.5, 0.5],\n)\n\n# Use DecisionTree instead of SkopeRules\noptions.template.learning_instantiator = XLEMOOSelectorOptions(\n    ml_model_type=\"DecisionTree\",\n    ml_model_kwargs={\"max_depth\": 4},\n    h_split=0.2,\n    l_split=0.2,\n    instantiation_factor=10.0,\n    unique_only=True,\n)\n\nsolver, extras = emo_constructor(emo_options=options, problem=problem)\nresults = solver()\n\nprint(f\"Solutions found: {len(results.optimal_outputs)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Generation lookback and ancestral recall\n",
    "\n",
    "By default, the learning mode uses the entire population history to train\n",
    "the ML model (`generation_lookback=0`). You can limit this to only recent\n",
    "generations, which may be useful when you want the ML model to focus on\n",
    "recently discovered good solutions.\n",
    "\n",
    "- `generation_lookback`: Only use the last N generations (0 = use all).\n",
    "- `ancestral_recall`: Always include the first N generations alongside\n",
    "  the lookback window, preserving early exploration diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "problem = dtlz2(n_objectives=3, n_variables=6)\n\noptions = xlemoo_options()\noptions.template.termination.max_generations = 60\noptions.template.darwin_iterations_per_cycle = 19\noptions.template.learning_iterations_per_cycle = 1\noptions.template.scalarization = ScalarizationSpec(\n    type=\"asf\",\n    symbol=\"scal_fitness\",\n    reference_point=[0.5, 0.5, 0.5],\n)\n\n# Only look at the last 20 generations, but always include the first 3\noptions.template.learning_instantiator = XLEMOOSelectorOptions(\n    ml_model_type=\"SkopeRules\",\n    ml_model_kwargs={\"precision_min\": 0.1, \"n_estimators\": 30},\n    h_split=0.2,\n    l_split=0.2,\n    instantiation_factor=10.0,\n    unique_only=True,\n    generation_lookback=20,\n    ancestral_recall=3,\n)\n\nsolver, extras = emo_constructor(emo_options=options, problem=problem)\nresults = solver()\n\nprint(f\"Solutions found: {len(results.optimal_outputs)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "## Comparing XLEMOO with standard IBEA\n\nA natural comparison is against standard IBEA on the same problem and budget.\nXLEMOO uses a `ScalarizationSelector` with ASF, while IBEA uses its native\nindicator-based selection."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "from desdeo.emo.options.algorithms import ibea_options, xlemoo_options\n\nproblem = dtlz2(n_objectives=3, n_variables=6)\nmax_gens = 100\n\n# Standard IBEA\nibea_opts = ibea_options()\nibea_opts.template.termination.max_generations = max_gens\nibea_solver, _ = emo_constructor(emo_options=ibea_opts, problem=problem)\nibea_results = ibea_solver()\n\n# XLEMOO with ASF\nxlemoo_opts = xlemoo_options()\nxlemoo_opts.template.termination.max_generations = max_gens\nxlemoo_opts.template.darwin_iterations_per_cycle = 19\nxlemoo_opts.template.learning_iterations_per_cycle = 1\nxlemoo_opts.template.scalarization = ScalarizationSpec(\n    type=\"asf\",\n    symbol=\"scal_fitness\",\n    reference_point=[0.5, 0.5, 0.5],\n)\nxlemoo_opts.template.learning_instantiator = XLEMOOSelectorOptions(\n    ml_model_type=\"SkopeRules\",\n    ml_model_kwargs={\"precision_min\": 0.1, \"n_estimators\": 30},\n    h_split=0.2,\n    l_split=0.2,\n    instantiation_factor=10.0,\n    unique_only=True,\n)\nxlemoo_solver, _ = emo_constructor(emo_options=xlemoo_opts, problem=problem)\nxlemoo_results = xlemoo_solver()\n\n# Compare distance to unit sphere\nibea_obj = ibea_results.optimal_outputs[[\"f_1\", \"f_2\", \"f_3\"]].to_numpy()\nxlemoo_obj = xlemoo_results.optimal_outputs[[\"f_1\", \"f_2\", \"f_3\"]].to_numpy()\n\nibea_norms = np.sqrt(np.sum(ibea_obj**2, axis=1))\nxlemoo_norms = np.sqrt(np.sum(xlemoo_obj**2, axis=1))\n\nprint(f\"{'':>20} {'IBEA':>10} {'XLEMOO':>14}\")\nprint(f\"{'Solutions found':>20} {len(ibea_obj):>10} {len(xlemoo_obj):>14}\")\nprint(f\"{'Median norm':>20} {np.median(ibea_norms):>10.4f} {np.median(xlemoo_norms):>14.4f}\")\nprint(f\"{'Mean norm':>20} {np.mean(ibea_norms):>10.4f} {np.mean(xlemoo_norms):>14.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter3d(\n        x=ibea_obj[:, 0], y=ibea_obj[:, 1], z=ibea_obj[:, 2],\n        mode=\"markers\",\n        marker=dict(size=3, color=\"blue\", opacity=0.6),\n        name=\"IBEA\",\n    )\n)\n\nfig.add_trace(\n    go.Scatter3d(\n        x=xlemoo_obj[:, 0], y=xlemoo_obj[:, 1], z=xlemoo_obj[:, 2],\n        mode=\"markers\",\n        marker=dict(size=3, color=\"red\", opacity=0.6),\n        name=\"XLEMOO-IBEA\",\n    )\n)\n\nfig.update_layout(\n    title=\"IBEA vs XLEMOO-IBEA on DTLZ2\",\n    scene=dict(xaxis_title=\"f_1\", yaxis_title=\"f_2\", zaxis_title=\"f_3\"),\n)\nfig.show(renderer=\"notebook\", include_plotlyjs=\"cdn\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Accessing the learning archive\n",
    "\n",
    "The `ConstructorExtras` object returned by `emo_constructor` includes a\n",
    "`learning_archive` that stores the full history of all evaluated solutions\n",
    "(from both Darwinian and Learning modes). This can be used for post-hoc\n",
    "analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "problem = dtlz2(n_objectives=3, n_variables=6)\n\noptions = xlemoo_options()\noptions.template.termination.max_generations = 50\noptions.template.darwin_iterations_per_cycle = 19\noptions.template.learning_iterations_per_cycle = 1\noptions.template.scalarization = ScalarizationSpec(\n    type=\"asf\",\n    symbol=\"scal_fitness\",\n    reference_point=[0.5, 0.5, 0.5],\n)\noptions.template.learning_instantiator = XLEMOOSelectorOptions(\n    ml_model_type=\"SkopeRules\",\n    ml_model_kwargs={\"precision_min\": 0.1, \"n_estimators\": 30},\n    h_split=0.2,\n    l_split=0.2,\n    instantiation_factor=10.0,\n    unique_only=True,\n)\n\nsolver, extras = emo_constructor(emo_options=options, problem=problem)\nresults = solver()\n\n# The learning archive contains all evaluated solutions with generation numbers\narchive_df = extras.learning_archive.solutions\nprint(f\"Total solutions in learning archive: {len(archive_df)}\")\nprint(f\"Generations covered: {archive_df['generation'].min()} to {archive_df['generation'].max()}\")\nprint(f\"\\nArchive columns: {archive_df.columns}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": "## Summary\n\nXLEMOO extends standard evolutionary algorithms with interpretable machine\nlearning to provide both optimization and explainability. Key takeaways:\n\n- Use `xlemoo_options()` for a ready-to-run configuration with a\n  `ScalarizationSelector` and weighted sums scalarization.\n- Configure the `ScalarizationSpec` on the template to choose between\n  `\"weighted_sums\"` and `\"asf\"` scalarization. Both the Darwinian selector\n  and the learning mode instantiator use the same scalarization column.\n- The **ASF** with a decision-maker-supplied reference point is the\n  recommended configuration (as in the original paper), guiding the learning\n  mode toward the preferred region of the Pareto front.\n- **SkopeRulesClassifier** is the recommended ML model for interpretable\n  rule extraction.\n- Use the `learning_archive` and rule extraction utilities for post-hoc\n  analysis and explainability.\n- XLEMOO supports interactive use: examine the extracted rules, then re-run\n  with a modified reference point to explore other regions."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}